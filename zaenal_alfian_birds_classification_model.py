# -*- coding: utf-8 -*-
"""Zaenal Alfian_Birds Classification Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CfdsJad5_FC7TQ8KX0vZj0VXGUxdbBF5

**NAMA: ZAENAL ALFIAN** <br>
**NIM: 202101251002**


-------

# **Datasets dari Kaggle**
"""

!pip install -q kaggle

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

!kaggle datasets list

! kaggle datasets download -d gpiosenka/100-bird-species

!mkdir hundred-birds
!unzip 100-bird-species.zip -d hundred-birds
!ls hundred-birds

"""# **Import library dan modules yang diperlukan**"""

import matplotlib.pyplot as plt
import numpy as np
import pathlib
import os
import tensorflow as tf
import tensorflow.keras.layers as tfl
from PIL import Image

from tensorflow.keras.preprocessing import image_dataset_from_directory
from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation

from sklearn.metrics import accuracy_score

data_dir = "hundred-birds"

IMAGE_SIZE = (160, 160)
BATCH_SIZE = 32

# checking a random image
sample_image = Image.open("hundred-birds/test/ALPINE CHOUGH/4.jpg")
print("Shape of the sample image: ", sample_image.size)
print("Image Mode: ", sample_image.mode)

"""# **Cek file gambar dan kelas datasets**"""

## Load the data using tf
def load_data(data_dir):

    train_images = tf.keras.utils.image_dataset_from_directory(
        os.path.join(data_dir, 'train'),
        image_size = IMAGE_SIZE,
        batch_size = BATCH_SIZE
    )

    dev_images = tf.keras.utils.image_dataset_from_directory(
        os.path.join(data_dir, 'test'),
        image_size = IMAGE_SIZE,
        batch_size = BATCH_SIZE
    )

    test_images = tf.keras.utils.image_dataset_from_directory(
        os.path.join(data_dir, 'valid'),
        image_size = IMAGE_SIZE,
        batch_size = BATCH_SIZE
    )

    return train_images, dev_images, test_images

train_dataset, dev_dataset, test_dataset = load_data(data_dir)

class_names = train_dataset.class_names

plt.figure(figsize=(10, 10))
for images, labels in train_dataset.take(1):
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title(class_names[labels[i]])
        plt.axis("off")

"""## **Preprocess dan Augmentasi Data**"""

AUTOTUNE = tf.data.experimental.AUTOTUNE
train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)

def data_augmenter():

    data_augmentation = tf.keras.models.Sequential()
    data_augmentation.add(RandomFlip('horizontal'))
    data_augmentation.add(RandomRotation(0.2))
    data_augmentation.add(tf.keras.layers.experimental.preprocessing.RandomZoom(0.1))

    return data_augmentation

data_augmentation = data_augmenter()

for image, _ in train_dataset.take(1):
    plt.figure(figsize=(10, 10))
    first_image = image[0]
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))
        plt.imshow(augmented_image[0] / 255)
        plt.axis('off')

preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input # preprocess transfer learning mobilenetv2

"""# **MobileNet V2 sebagai Basemodel**"""

IMG_SHAPE = IMAGE_SIZE + (3,)

base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

base_model.summary()

def make_model(image_shape, data_augmetation, base_model):

    input_shape = image_shape + (3,)

    inputs = tf.keras.Input(shape=input_shape)
    x = data_augmetation(inputs)
    x = preprocess_input(x)
    x = base_model(x, training=False)
    x = tfl.GlobalMaxPooling2D()(x)
    output = tfl.Dense(525)(x)

    model = tf.keras.Model(inputs, output)

    return model

model = make_model(IMAGE_SIZE, data_augmenter(), base_model)
model.summary()

loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
optimizer = tf.keras.optimizers.Adam(learning_rate=0.1*0.001)
metrics=['accuracy']

model.compile(loss=loss_function,
              optimizer = optimizer,
              metrics=metrics)

# Unfreezing the last few layers
base_model.trainable = True

total_layers = len(base_model.layers)
print("Total Layers In Base Model: ", total_layers)

fine_tune_at = 120

# unfreeze rest of the layers
for layers in base_model.layers[:fine_tune_at]:
    layers.trainable=False

model.summary()

"""# **Tahap Training Model**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history = model.fit(train_dataset,
#                     validation_data=dev_dataset,
#                     epochs=10)

"""# **Menampilkan performa akurasi model**"""

acc = [0.] + history.history['accuracy']
val_acc = [0.] + history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.5])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

# Tes pada test datasets
pred = model.predict(test_dataset)

labels_array = np.array([])
pred_array = np.array([])

for x, y in test_dataset.take(1):
    pred_prob = model.predict(x)
    labels_array = np.concatenate([labels_array, y])
    pred_class = np.argmax(pred_prob, axis=1)
    pred_array = np.concatenate([pred_array, pred_class])

"""# **Mengecek tingkat akurasi pada tes**"""

print("Accuracy On Test Dataset: ", accuracy_score(labels_array, pred_array))

"""# **Deploy model TFLite**"""

export_dir = 'saved_model/'
tf.saved_model.save(model, export_dir)

converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)
tflite_model = converter.convert()

tflite_model_file = pathlib.Path('uas.tflite')
tflite_model_file.write_bytes(tflite_model)